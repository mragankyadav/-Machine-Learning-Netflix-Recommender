{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Embeddings (50 points)\n",
    "For this first part, we're going to implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters.\n",
    "\n",
    "If you're interested in a deeper treatment of comparing count vs. learned embeddings, take a look at: [Donâ€™t count, predict! A systematic comparison of\n",
    "context-counting vs. context-predicting semantic vectors](\n",
    "http://www.aclweb.org/anthology/P14-1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us, you can find the Brown corpus in nltk. *Make sure you have already installed nltk with something like: conda install nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/Mragank/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it locally, you can load the dataset into your notebook. You can access the words using brown.words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Mragank/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "processed_corpus=[]\n",
    "for words in brown.sents():\n",
    "    sentence=' '.join(words)\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    processed_corpus.append(filtered_words)\n",
    "# print processed_corpus[0]\n",
    "# print len(processed_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent 20 words in Corpus\n",
      "one\n",
      "would\n",
      "said\n",
      "time\n",
      "new\n",
      "could\n",
      "two\n",
      "may\n",
      "first\n",
      "man\n",
      "like\n",
      "even\n",
      "made\n",
      "also\n",
      "many\n",
      "must\n",
      "well\n",
      "af\n",
      "back\n",
      "years\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "word_count={}\n",
    "for sent in processed_corpus:\n",
    "    for word in sent:\n",
    "        word_count[word]=word_count.get(word,0)+1\n",
    "sorted_words = sorted(word_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "vocabulary=[]\n",
    "context=[]\n",
    "topTwenty=[]\n",
    "for i in range(5000):\n",
    "    vocabulary.append(sorted_words[i][0])\n",
    "    if i <1000:\n",
    "        context.append(sorted_words[i][0])\n",
    "    if i <20:\n",
    "        topTwenty.append(sorted_words[i][0])\n",
    "print \"Most Frequent 20 words in Corpus\"\n",
    "for i in topTwenty:       \n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building complete context for all vocabulary words\n",
    "word_to_context={}\n",
    "for sentence in brown.sents():\n",
    "    for idx in range(len(sentence)):\n",
    "        if sentence[idx].lower() in vocabulary:\n",
    "            surroundings=word_to_context.get(sentence[idx].lower(),{})\n",
    "            for window in range(max(idx-2,0),min(idx+3,len(sentence))):\n",
    "                surroundings[sentence[window].lower()]=surroundings.get(sentence[window].lower(),0)+1\n",
    "            word_to_context[sentence[idx].lower()]=surroundings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#building co-ocurrence matrix from the complete context dictionary created above\n",
    "co_occurence_matrix=[[0 for i in range(len(context))]for j in range(len(vocabulary))]\n",
    "nf_count=0\n",
    "nf_d={}\n",
    "for v in range(len(vocabulary)):\n",
    "    for c in range(len(context)):\n",
    "        raw_context=word_to_context.get(vocabulary[v],None)\n",
    "        if raw_context:\n",
    "            co_occurence_matrix[v][c]=raw_context.get(context[c],0)\n",
    "        else:\n",
    "            co_occurence_matrix[v][c]=0\n",
    "# print nf_count  \n",
    "# for i in range(10):\n",
    "#     print co_occurence_matrix[i]\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# creating both Pr(c|w) and Pr(c) matrix and array\n",
    "pr_cw=[[0 for i in range(len(context))]for j in range(len(vocabulary))]\n",
    "pr_c=[0 for i in range(len(context))]\n",
    "total_context_frequency=0\n",
    "\n",
    "#Added Laplace smoothing by 1 to avoid division by 0 issues\n",
    "for v in range(len(vocabulary)):\n",
    "    total_context_frequency+=sum(co_occurence_matrix[v])+len(context)\n",
    "    for c in range(len(context)):\n",
    "        pr_cw[v][c]=co_occurence_matrix[v][c]+1/(sum(co_occurence_matrix[v])+len(context))\n",
    "for c in range(len(context)):\n",
    "    for v in range(len(vocabulary)):\n",
    "        pr_c[c]+= co_occurence_matrix[v][c]+1\n",
    "    pr_c[c]/=total_context_frequency\n",
    "    \n",
    "# print pr_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now you can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "word_vectors=[[0 for i in range(len(context))]for j in range(len(vocabulary))]\n",
    "for v in range(len(vocabulary)):\n",
    "    for c in range(len(context)):\n",
    "        word_vectors[v][c]=max(0,math.log10(pr_cw[v][c]/pr_c[c]))\n",
    "# print word_vectors[1]\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, you should:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering into 100 clusters\n",
    "import numpy as np  \n",
    "from sklearn.cluster import KMeans  \n",
    "word_vectors = np.asarray(word_vectors)\n",
    "kmeans = KMeans(n_clusters=100)  \n",
    "kmeans.fit(word_vectors)\n",
    "word_clusters={}\n",
    "for label in range(len(kmeans.labels_)):\n",
    "    cluster= word_clusters.get(kmeans.labels_[label],[])\n",
    "    cluster.append(vocabulary[label])\n",
    "    word_clusters[kmeans.labels_[label]]=cluster\n",
    "# Uncomment this to print all clusters    \n",
    "# for k,v in word_clusters.iteritems():\n",
    "#     print word_clusters[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three nearest neighbours of \"one\":\n",
      "would new another\n",
      "\n",
      "Three nearest neighbours of \"would\":\n",
      "one could may\n",
      "\n",
      "Three nearest neighbours of \"said\":\n",
      "one would like\n",
      "\n",
      "Three nearest neighbours of \"time\":\n",
      "one would man\n",
      "\n",
      "Three nearest neighbours of \"new\":\n",
      "one would may\n",
      "\n",
      "Three nearest neighbours of \"could\":\n",
      "would one may\n",
      "\n",
      "Three nearest neighbours of \"two\":\n",
      "one many three\n",
      "\n",
      "Three nearest neighbours of \"may\":\n",
      "would one must\n",
      "\n",
      "Three nearest neighbours of \"first\":\n",
      "one would new\n",
      "\n",
      "Three nearest neighbours of \"man\":\n",
      "one time would\n",
      "\n",
      "Three nearest neighbours of \"like\":\n",
      "would one could\n",
      "\n",
      "Three nearest neighbours of \"even\":\n",
      "one would may\n",
      "\n",
      "Three nearest neighbours of \"made\":\n",
      "one would make\n",
      "\n",
      "Three nearest neighbours of \"also\":\n",
      "would one may\n",
      "\n",
      "Three nearest neighbours of \"many\":\n",
      "two one would\n",
      "\n",
      "Three nearest neighbours of \"must\":\n",
      "would may one\n",
      "\n",
      "Three nearest neighbours of \"well\":\n",
      "would one even\n",
      "\n",
      "Three nearest neighbours of \"af\":\n",
      "one may would\n",
      "\n",
      "Three nearest neighbours of \"back\":\n",
      "one would go\n",
      "\n",
      "Three nearest neighbours of \"years\":\n",
      "days year months\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nearest Neighbours for top 20 most frequest words\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neighbours= NearestNeighbors(n_neighbors=4, radius=1.0, \n",
    "    algorithm='auto', metric='cosine')\n",
    "neighbours.fit(word_vectors)\n",
    "top_twenty=[]\n",
    "for i in range(20):\n",
    "    top_twenty.append(word_vectors[i])\n",
    "# print neighbours.kneighbors(top_twenty)\n",
    "three_neighbours= neighbours.kneighbors(top_twenty, return_distance=False)\n",
    "for i in range(20):\n",
    "    print \"Three nearest neighbours of \\\"\"+vocabulary[i]+\"\\\":\"\n",
    "    print vocabulary[three_neighbours[i][1]],\n",
    "    print vocabulary[three_neighbours[i][2]],\n",
    "    print vocabulary[three_neighbours[i][3]]\n",
    "    print\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Clustering Observation</b>\n",
    "\n",
    "Note: The part of code mentioned with a comment can be uncommented to print all 100 clusters.\n",
    "    \n",
    "After clustering into 100 clusters and printing the words of all the 100 clusters, I was able to observe that\n",
    "similar words were clustered together in one group.\n",
    "We can see that with the help of following example clusters I found:\n",
    "\n",
    "Example 1:\n",
    "[u'black', u'red', u'brown', u'dark', u'kept'...]\n",
    "\n",
    "All colors from the text were in same cluster\n",
    "\n",
    "Example 2:\n",
    "[u'1', u'2', u'3', u'per', u'4', u'5', u'10', u'figure', u'6', u'8', u'1960', u'7', u'30',\n",
    " u'cent', u'15', u'20', u'12', u'nearly', u'1961', u'50', u'25', u'100'..]:\n",
    " \n",
    " All numbers from the text were in the same cluster\n",
    "\n",
    "Example 3:\n",
    "[u'united', u'form', u'case', u'family', u'members'..]\n",
    "\n",
    "All words related to group/collection of people were clustered in the same group.\n",
    "\n",
    "There were also some words clutered together that were not very \n",
    "similar in meaning but I think the reason for that is pretty obvious. Words were clustered based on their cluster centers\n",
    "that were derived from their vector representation. We only used 1000 context words and 5000 vocabulary to define \n",
    "every word as a vector. Not only is the size of the vector small in terms of dimension but also the vectors are sparse.\n",
    "Also to determine the vector values, we used basic probability distribution which may not be the best measure to create\n",
    "mutual information vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Nearest Neighbour Observation</b>\n",
    "\n",
    "The results of 3 nearest neighbours of all the most frequent 20 words has been printed above. \n",
    "We observe that most of the neighbours are dominated by the top 100 or top 150 most frequent words in the corpus. I belive this may be because of the distance function that we have used to find the closest neighbours. We have cosine similarity which is highly biased towards the bigger vectors. Also the closest aren't totally irrelevant and some of them are proper but I believe that can be improved if I used a better distance function or used some normalisation technique to offset the effect of very frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Collaborative Filtering (50 points)\n",
    "\n",
    "In this second part, you will implement collaborative filtering on the Netflix prize dataset -- donâ€™t freak out, the provided sample dataset has only ~2000 items and ~28,000 users.\n",
    "\n",
    "As background, read the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) up to Section 2.1. Of course you can read further if you are interested, and you can also refer to the course slides for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "Total Samples in Training Set=3255352\n",
      "Total unique users in Training Set=28978\n",
      "Total unique movies in Training Set=1821\n",
      "Total unique ratings in Training Set=5\n",
      "\n",
      "TESTING SET\n",
      "Total Samples in Testing Set=3255352\n",
      "Total unique users in Testing Set=27555\n",
      "Total unique movies in Testing Set=1701\n",
      "Total unique ratings in Testing Set=5\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "training_data=[]\n",
    "testing_data=[]\n",
    "\n",
    "#Reading Training Data\n",
    "with codecs.open(\"/Users/mragank/Desktop/netflix-dataset/TrainingRatings.txt\", encoding=\"utf-8\") as f:\n",
    "    try:\n",
    "        for row in  f.readlines():\n",
    "            training_data.append(row.split(','))\n",
    "    finally:\n",
    "        f.close()\n",
    "\n",
    "#Reading Testing Data\n",
    "with codecs.open(\"/Users/mragank/Desktop/netflix-dataset/TestingRatings.txt\", encoding=\"utf-8\") as f:\n",
    "    try:\n",
    "        for row in  f.readlines():\n",
    "            testing_data.append(row.split(','))\n",
    "    finally:\n",
    "        f.close()\n",
    "\n",
    "#Printing Training Data Details\n",
    "tr_users={}\n",
    "users_count=0\n",
    "tr_movies={}\n",
    "movies_count=0\n",
    "tr_ratings={}\n",
    "print \"TRAINING SET\"\n",
    "print \"Total Samples in Training Set=\"+str(len(training_data))\n",
    "for sample in training_data:\n",
    "    if tr_movies.get(sample[0],None)==None:\n",
    "        tr_movies[sample[0]]=movies_count\n",
    "        movies_count+=1\n",
    "    if tr_users.get(sample[1],None)==None:\n",
    "        tr_users[sample[1]]=users_count\n",
    "        users_count+=1\n",
    "    if tr_ratings.get(sample[2],None)==None:\n",
    "        tr_ratings[sample[2]]=True\n",
    "print \"Total unique users in Training Set=\"+str(len(tr_users))\n",
    "print \"Total unique movies in Training Set=\"+str(len(tr_movies))\n",
    "print \"Total unique ratings in Training Set=\"+str(len(tr_ratings))\n",
    "print\n",
    "\n",
    "#Printing Testing Data Details\n",
    "tst_users={}\n",
    "users_count=0\n",
    "tst_movies={}\n",
    "movies_count=0\n",
    "tst_ratings={}\n",
    "print \"TESTING SET\"\n",
    "print \"Total Samples in Testing Set=\"+str(len(training_data))\n",
    "for sample in testing_data:\n",
    "    if tst_movies.get(sample[0],None)==None:\n",
    "        tst_movies[sample[0]]=movies_count\n",
    "        movies_count+=1\n",
    "    if tst_users.get(sample[1],None)==None:\n",
    "        tst_users[sample[1]]=users_count\n",
    "        users_count+=1\n",
    "    if tst_ratings.get(sample[2],None)==None:\n",
    "        tst_ratings[sample[2]]=True\n",
    "print \"Total unique users in Testing Set=\"+str(len(tst_users))\n",
    "print \"Total unique movies in Testing Set=\"+str(len(tst_movies))\n",
    "print \"Total unique ratings in Testing Set=\"+str(len(tst_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Creating User to Movies Rating Matrix from the input data\n",
    "training_matrix=[[-1 for i in range(len(tr_movies))]for j in range(len(tr_users))]\n",
    "testing_matrix=[[-1 for i in range(len(tst_movies))]for j in range(len(tst_users))]\n",
    "for sample in training_data:\n",
    "    training_matrix[tr_users[sample[1]]][tr_movies[sample[0]]]=float(sample[2])\n",
    "    \n",
    "for sample in testing_data:\n",
    "    testing_matrix[tst_users[sample[1]]][tst_movies[sample[0]]]=float(sample[2])\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Recommendation Algorithm\n",
    "from __future__ import division\n",
    "average_ratings=[]\n",
    "for user in range(len(training_matrix)):\n",
    "    va=0\n",
    "    count=0\n",
    "    for movie in range(len(training_matrix[0])):\n",
    "        if training_matrix[user][movie]!=-1:\n",
    "            va+=training_matrix[user][movie]\n",
    "            count+=1\n",
    "    average_ratings.append(va/count)\n",
    "# print average_ratings\n",
    "print \"Done\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Done\n"
     ]
    }
   ],
   "source": [
    "real_rating=[]\n",
    "predicted_rating=[]\n",
    "for sample in range(5000):\n",
    "    user= testing_data[sample][1]\n",
    "    movie=testing_data[sample][0]\n",
    "    real_rating.append(float(testing_data[sample][2]))\n",
    "    \n",
    "    if tr_users.get(user,None)!=None and tr_movies.get(movie,None)!=None:\n",
    "        va = average_ratings[tr_users[user]]\n",
    "        k=0.001\n",
    "        temp=0\n",
    "        for i in range(len(training_matrix)):\n",
    "            wai=0\n",
    "            vajss=0\n",
    "            vijss=0\n",
    "            for j in range(len(training_matrix[0])):\n",
    "                if training_matrix[tr_users[user]][j]!=-1 and training_matrix[i][j]!=-1:\n",
    "                    vaj=training_matrix[tr_users[user]][j]-average_ratings[tr_users[user]]\n",
    "                    vij=training_matrix[i][j]-average_ratings[i]\n",
    "                    vajss+= vaj**2\n",
    "                    vijss+= vij**2\n",
    "                    wai+=(vaj*vij)\n",
    "            if vajss!=0 and vijss!=0:\n",
    "                wai/= (vajss*vijss)**0.5\n",
    "            curr_movie_rating=0\n",
    "            if training_matrix[i][tr_movies[movie]]!=-1:\n",
    "                curr_movie_rating=training_matrix[i][tr_movies[movie]]\n",
    "                temp+=(wai*(curr_movie_rating-average_ratings[i]))\n",
    "#         print wai,\n",
    "#         print temp\n",
    "        temp*=k\n",
    "        pa=va+temp\n",
    "        predicted_rating.append(pa)\n",
    "                    \n",
    "    else:\n",
    "        predicted_rating.append(sum(average_ratings)/len(average_ratings))\n",
    "    if sample%25==0:\n",
    "        print \".\",\n",
    "# print predicted_rating\n",
    "# print real_rating[:10]\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7248858378559189\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print mean_absolute_error(real_rating, predicted_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8540646831590417\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print mean_squared_error(real_rating, predicted_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest neigbour Approach for Rating Prediction\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh= NearestNeighbors(n_neighbors=100, radius=1.0, algorithm='auto', metric='cosine')\n",
    "neigh.fit(training_matrix)\n",
    "\n",
    "pr=[]\n",
    "rr=[]\n",
    "test_data=[]\n",
    "movie_data=[]\n",
    "for sample in range(5000):\n",
    "    user= testing_data[sample][1]\n",
    "    movie=testing_data[sample][0]\n",
    "    rr.append(float(testing_data[sample][2]))\n",
    "    test_data.append(training_matrix[tr_users[user]])\n",
    "    movie_data.append(movie)\n",
    "    \n",
    "# print neighbours.kneighbors(top_twenty)\n",
    "fifty_neighbours= neigh.kneighbors(test_data, return_distance=False)\n",
    "# print fifty_neighbours\n",
    "# print training_data[1]\n",
    "for sample in range(len(fifty_neighbours)):\n",
    "    rating=0\n",
    "    count=0\n",
    "    for neighbour in range(1,len(fifty_neighbours[sample])):\n",
    "        if training_matrix[fifty_neighbours[sample][neighbour]][tr_movies[movie_data[sample]]]!=-1:\n",
    "            rating+=training_matrix[fifty_neighbours[sample][neighbour]][tr_movies[movie_data[sample]]]\n",
    "            count+=1\n",
    "    if count!=0:\n",
    "        pr.append(rating/count)\n",
    "    else:\n",
    "        pr.append(average_ratings[tr_users[user]])\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803930816596558\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print mean_absolute_error(rr, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.027159146621389\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print mean_squared_error(rr, pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for improvements on the pearson correlation approach implemented above, I tried two different approaches on the complete 5000 users data set. Here are the approaches and their results:\n",
    "\n",
    "Approach 1: K-Means Clustering<br>\n",
    "I formed 500 clusters based on user similarity. I used the user-item matrix to form user vectors and used those vectors to group similar users. Once the clusters were formed, I found the clusters of the user in question and checked how many users in the cluster have rated the movie in question. I calculated the average rating for that movie by similar users in the clusters and assigned that rating to the movie. Here are the MAE and RMSE values obtained from this approach:<br>\n",
    "MAE: 0.846841002769792<br>\n",
    "RMSE: 1.2108123167172067<br>\n",
    "\n",
    "Approach 2: N-Nearest Neigbours<br>\n",
    "Since the results by this approach weren't highly optimal, I tried the second approach of nearest neighbours. I again formed user vectors from user-item matrix and found the 100 nearest neighbours for every user being tested. Once I found the neighbours, I checked all the neighbours that have rated the movie in question and calculated the average movie rating for the givem movie. This approach significantly improved the MAE and RMSE values in comparison to the clustering techniques and the results are as follows:<br>\n",
    "MAE: 0.803930816596558<br>\n",
    "RMSE: 1.027159146621389<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
